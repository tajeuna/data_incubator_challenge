{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from finch import FINCH\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.cluster import *\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import * #DotProduct, WhiteKernel\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('temp_datalab_records_linkedin_company.csv',engine='c', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_frame(df,chunk_size):\n",
    "    '''\n",
    "        Splitting the dataframe into sub-dataframes of sizes chunk_size.\n",
    "        Return the window-stamps (intervals) and the list of sub-dataframes\n",
    "        :params df, chunk_size: dataframe and size of sub-dataframes\n",
    "    '''\n",
    "    def index_marks(): return range(1 * chunk_size, (len(df) // chunk_size + 1) * chunk_size, chunk_size)\n",
    "        \n",
    "    indices = index_marks()\n",
    "    window_stamps = []\n",
    "    frames = np.split(df,indices)\n",
    "    dico = {}\n",
    "    for i in range(len(indices)):\n",
    "        dico['$W^'+str(chunk_size)+'_'+str(i+1)+'$'] = frames[i]\n",
    "        window_stamps.append('$W^'+str(chunk_size)+'_'+str(i+1)+'$')\n",
    "    \n",
    "    return window_stamps, dico\n",
    "\n",
    "def added_vals(list_values):\n",
    "    '''\n",
    "        Calculate the differentiation with lag == 1\n",
    "    '''\n",
    "    current = []\n",
    "    for i in range(len(list_values)):\n",
    "        if i == 0: current.append(0)\n",
    "        else: current.append(list_values[i]-list_values[i-1])\n",
    "    return current\n",
    "\n",
    "def company_information(df,company):\n",
    "    '''\n",
    "        get the number of followers per day for a given company.\n",
    "    '''\n",
    "    \n",
    "    df2 = df[df.company_name == company]\n",
    "    df2['added_followers_count'] = added_vals(df2.followers_count.values)\n",
    "    df2['added_employees_on_platform'] = added_vals(df2.employees_on_platform.values)\n",
    "    return df2.loc[:, ['as_of_date','company_name','followers_count',\n",
    "                       'added_followers_count','employees_on_platform','added_employees_on_platform']]\n",
    "\n",
    "def get_all_company_followers(df,companies):\n",
    "    '''\n",
    "        Get the overall company number of followers\n",
    "        :params df, companies: dataframe and the list of companies\n",
    "    '''\n",
    "    list_frames = []\n",
    "    #list_frames = Parallel(n_jobs=2)(delayed(company_information)(df,c) for c in companies)\n",
    "    for c in companies:\n",
    "        info = company_information(df,c)\n",
    "        info[c] = info.added_followers_count\n",
    "        info.set_index('as_of_date',inplace=True)\n",
    "        list_frames.append(info.loc[:,[c]])\n",
    "    whole_frame = pd.concat(list_frames, axis=1)\n",
    "    whole_frame.fillna(value=0,inplace=True)\n",
    "    return whole_frame\n",
    "\n",
    "def scale_series(df,range_interval):\n",
    "    \n",
    "    '''\n",
    "        Scaled in dataframe information within a range interval\n",
    "        :params df, range_interval: dataframe and tuple of 2 values (x,y), x<y\n",
    "    '''\n",
    "    \n",
    "    def scale_data(data):\n",
    "        scaler = MinMaxScaler(feature_range=range_interval)\n",
    "        scaler = scaler.fit(data)\n",
    "        scaled = scaler.transform(data)\n",
    "        return scaled\n",
    "    \n",
    "    df2 = deepcopy(df)\n",
    "    \n",
    "    for h in list(df):\n",
    "        df2[h] = scale_data(np.matrix(df[h]).T)\n",
    "        \n",
    "    return df2\n",
    "\n",
    "def clustering(df):\n",
    "    #FINCH clustering\n",
    "    return FINCH(np.array(df),verbose=False)\n",
    "\n",
    "def get_nbre_components2(df):\n",
    "    return len(FINCH(np.array(df),verbose=False)[1])\n",
    "\n",
    "def scanning2(list_frames):\n",
    "    '''\n",
    "        Calculate the variation score from a according to the list of dataframes\n",
    "    '''\n",
    "    pool = mp.Pool()\n",
    "    results = pool.map(get_nbre_components2, list_frames)\n",
    "    pool.terminate()\n",
    "    dyn = max([results[i] for i in range(len(results))]) / len(list(list_frames[0]))\n",
    "    return dyn\n",
    "\n",
    "def get_nbre_components3(df,k,seed):\n",
    "    X = np.array(df)\n",
    "    gmm = mixture.GaussianMixture(n_components = k,covariance_type='full',random_state=seed)\n",
    "    gmm.fit(X)\n",
    "    means = gmm.means_\n",
    "    variances = gmm.covariances_\n",
    "        \n",
    "    return means, variances\n",
    "\n",
    "def entropy_cut_off(scores):\n",
    "    '''\n",
    "        Identifying a threshold from a given list of scores\n",
    "    '''\n",
    "    E = {}\n",
    "    F = {}\n",
    "    entropy_E = {}\n",
    "    entropy_F = {}\n",
    "    code = {}\n",
    "    result = None\n",
    "    for i in range(2,len(scores)):\n",
    "        E_temp = scores[0:i]\n",
    "        F_temp = scores[i:len(scores)]\n",
    "        entropy_E[i-1] = stats.entropy(E_temp)/len(E_temp)\n",
    "        entropy_F[i] = stats.entropy(F_temp)/len(F_temp)\n",
    "        E[i] = E_temp\n",
    "        F[i] = F_temp\n",
    "        code[i] = np.abs(entropy_E[i-1] - entropy_F[i])\n",
    "    minim = min(list(code.values()))\n",
    "    for i in sorted(list(code.keys())):\n",
    "        if code[i]<0.042:\n",
    "            result = (i,E[i],F[i],code[i])\n",
    "            break\n",
    "    return result, code, entropy_E, entropy_F\n",
    "\n",
    "def plot_company_followers(df,company):\n",
    "    \n",
    "    company_info = company_information(df,company).loc[:, ['as_of_date','added_followers_count']]\n",
    "    company_info.set_index('as_of_date',inplace=True)\n",
    "    fig = plt.figure(figsize=(12,4),dpi=125)\n",
    "    axe = fig.add_subplot(111)\n",
    "    company_info.plot(ax=axe, lw=1.5,alpha=1)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_company_employees(df,company):\n",
    "    \n",
    "    company_info = company_information(df,company).loc[:, ['as_of_date','added_employees_on_platform']]\n",
    "    company_info.set_index('as_of_date',inplace=True)\n",
    "    fig = plt.figure(figsize=(12,4),dpi=125)\n",
    "    axe = fig.add_subplot(111)\n",
    "    company_info.plot(ax=axe, lw=1.5,alpha=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = list(data.company_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = data.as_of_date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = get_all_company_followers(data,companies[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_whole_data = scale_series(whole_data,(0,1))\n",
    "scaled_whole_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = []\n",
    "nbre_components = {}\n",
    "dynamic = {}\n",
    "step_forward = 7\n",
    "size = 5\n",
    "cpt = 0\n",
    "window_sizes.append(size)\n",
    "window_instances, dico_fr = window_frame(scaled_whole_data,size)\n",
    "dyn = scanning2([fr.T for fr in dico_fr.values()])\n",
    "dynamic[size] = dyn\n",
    "print(dyn, end=';')\n",
    "while size<len(scaled_whole_data):\n",
    "    size += step_forward\n",
    "    window_sizes.append(size)\n",
    "    window_instances, dico_fr = window_frame(scaled_whole_data,size)\n",
    "    #window_instances, dico_fr = get_overlapping_window_instances2(test,size,size//2)\n",
    "    current_dyn = scanning2([fr.T for fr in dico_fr.values()])\n",
    "    dynamic[size] = current_dyn\n",
    "    print(current_dyn, end=';')\n",
    "    if np.abs(current_dyn - dyn)<.001:\n",
    "        break\n",
    "    else:\n",
    "        dyn = deepcopy(current_dyn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold identification\n",
    "\n",
    "fig = plt.figure(figsize=(4,4),dpi=150)\n",
    "sns.set_style('ticks')\n",
    "axe = fig.add_subplot(111)\n",
    "axe.set_facecolor('white')\n",
    "axe.plot(window_sizes, [dynamic[s] for s in window_sizes],'k-o',lw=.7,alpha=1,markersize=3)\n",
    "result, code, entropy_E, entropy_F = entropy_cut_off(np.array([dynamic[s] for s in window_sizes]))\n",
    "#result, code, entropy_E, entropy_F = entropy_cut_off(np.array(touse))\n",
    "\n",
    "tresh = window_sizes[result[0]]\n",
    "\n",
    "\n",
    "axe.vlines(x=tresh,ymin=0,ymax=max(list(dynamic.values()))-.1,\n",
    "           color='red',label='Optimal window: '+str(tresh))\n",
    "axe.xaxis.set_tick_params(direction='in',top=False,bottom=True,length=8,color='k',labelsize=13,pad=2)\n",
    "axe.yaxis.set_tick_params(direction='in',top=False,bottom=True,length=8,color='k',labelsize=13,pad=2)\n",
    "axe.minorticks_on()\n",
    "axe.set_ylabel('Series variation',weight='semibold',fontsize=18)\n",
    "axe.set_xlabel('Window size',weight='semibold',fontsize=18)\n",
    "axe.legend(loc='upper right',fontsize=20,prop={'weight':'bold'})\n",
    "plt.xticks(weight='semibold')\n",
    "plt.yticks(weight='semibold')\n",
    "axe.text(7,.6,'Sparse area',color='darkgreen',fontsize=10,weight='semibold')\n",
    "axe.text(tresh+5,.2,'Dense area',color='darkred',fontsize=10,weight='semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_variance(rho,data,nbre_reg,seed):\n",
    "    window_instances2, dico_fr2 = window_frame(data,rho)\n",
    "    tpfr = dico_fr2[window_instances2[0]]\n",
    "    tpfr.index = np.arange(len(tpfr))\n",
    "    head = list(tpfr)\n",
    "    head2 = deepcopy(head)\n",
    "    for h in head2:\n",
    "        arr = np.array(tpfr[h])\n",
    "    taje = tpfr.loc[:, head]\n",
    "    for i in range(1,len(window_instances2)):\n",
    "        tpfr = dico_fr2[window_instances2[i]]\n",
    "        tpfr.index = np.arange(len(tpfr))\n",
    "        head = list(tpfr)\n",
    "        head2 = deepcopy(head)\n",
    "        for h in head2:\n",
    "            arr = np.array(tpfr[h])\n",
    "        tpfr = tpfr.loc[:, head]\n",
    "        taje = pd.concat([taje,tpfr],axis=1)\n",
    "    mean,variance = get_nbre_components3(taje.T,nbre_reg,seed)\n",
    "    return mean,variance\n",
    "def get_final_regimes(mean,variance,seed):\n",
    "    patterns = {}\n",
    "    cpt = 1\n",
    "    for i in range(len(mean)):\n",
    "        m,v = mean[i,:],variance[i]\n",
    "        vals = stats.multivariate_normal.rvs(m,v,size=1,random_state=seed)\n",
    "        patterns['R'+str(cpt)+'_'+str(i)] = vals\n",
    "        cpt += 1\n",
    "    patterns = pd.DataFrame(patterns)\n",
    "    new_patterns = scale_series(patterns, (1,2))\n",
    "    g = get_network_from_series(new_patterns)[0]\n",
    "    print(g.degree())\n",
    "    print(np.median(g.degree()))\n",
    "    index = np.where(np.array(g.degree())<=np.median(g.degree()))\n",
    "    if len(index[0]) == len(list(patterns)):\n",
    "        return patterns\n",
    "    else:\n",
    "        final_patterns = {}\n",
    "        cpt = 1\n",
    "        for i in index[0]:\n",
    "            arr = np.array(patterns[list(patterns)[i]])\n",
    "            final_patterns['R'+str(cpt)+'_'+str(i)] = arr\n",
    "            cpt +=1\n",
    "        final_patterns = pd.DataFrame(final_patterns)\n",
    "        return final_patterns\n",
    "\n",
    "rho = 24\n",
    "K = int(result[1][len(result[1])-1]*rho)\n",
    "print(K)\n",
    "seed = 11\n",
    "mean,variance = get_mean_variance(rho,test,K,seed)\n",
    "final_patterns = get_final_regimes(mean,variance,seed)\n",
    "    \n",
    "fig = plt.figure(figsize=(10,2),dpi=125)\n",
    "for i in range(len(list(final_patterns))):\n",
    "    sns.set_style('ticks')\n",
    "    axe = fig.add_subplot(1,len(list(final_patterns)),i+1)\n",
    "    show = scale_series(final_patterns, (0,1))\n",
    "    show[list(final_patterns)[i]].plot(color='k',linestyle='-',lw=.75,alpha=1,ax=axe)\n",
    "    #axe.vlines(x=0,ymin=min(arr),ymax=max(arr),linestyles='-',lw=1,alpha=1)\n",
    "    #axe.hlines(y=min(arr),xmin=0,xmax=75,linestyles='-',lw=1,alpha=1)\n",
    "    axe.xaxis.set_tick_params(direction='in',top=False,bottom=True,length=2,color='k',labelsize=8,pad=2)\n",
    "    axe.yaxis.set_tick_params(direction='in',top=False,bottom=True,length=2,color='k',labelsize=8,pad=2)\n",
    "    axe.set_facecolor('white')\n",
    "    plt.xticks(weight='semibold')\n",
    "    plt.yticks(weight='semibold')\n",
    "    if i == 0: axe.set_ylabel('COmpanies',weight='semibold')\n",
    "plt.subplots_adjust(wspace=.23)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
